<div align="center">
<h1>Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function</h1>

[Chenyi Zhuang](https://chenyi-zhuang.github.io/), [Ying Hu], [Pan Gao](https://i2-multimedia-lab.github.io/members.html#:~:text=Pan%20Gao.%20I%20am%20currently%20an%20Associate%20Professor%20at%20the)

<div>
    <sup></sup>[I2ML](https://i2-multimedia-lab.github.io/), Nanjing University of Aeronautics and Astronautics
</div>

[Paper]()

<p><B>we propose Magnet, a training-free approach that improves attribute binding by manipulating object embeddings, enhancing disentanglement within the textual space.</B></p>

<img src="./figures/magnet_workflow.jpg" width="800px">

<p align="justify">Given the prompt, Magnet estimates positive and negative vectors and modifies the text embedding of each object.</p>

## TODO
- [x] Release the source code and model.
- [x] Extend to more T2I models.
- [x] Extend to controlling approaches.

</div>
